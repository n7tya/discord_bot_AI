import json
import os
from datetime import datetime
import asyncio
import random

# PyTorchã¨Transformersã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®‰å…¨ã«è¡Œã†
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TORCH_AVAILABLE = True
except ImportError as e:
    print(f"PyTorch/Transformersã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    TORCH_AVAILABLE = False

class LocalAI:
    def __init__(self):
        # ã‚ˆã‚Šè»½é‡ãªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        self.model_name = "rinna/japanese-gpt2-medium"
        
        if TORCH_AVAILABLE:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = None
        
        # å®Ÿéš›ã®AIãƒ¢ãƒ‡ãƒ«ã‚’æœ‰åŠ¹åŒ–ï¼ˆPyTorchãŒåˆ©ç”¨ã§ãã‚‹å ´åˆï¼‰
        self.use_real_model = TORCH_AVAILABLE
        
        try:
            if self.use_real_model and TORCH_AVAILABLE:
                # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
                print(f"æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« {self.model_name} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    use_fast=True
                )
                
                # pad_tokenã®è¨­å®š
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                
                # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                if not torch.cuda.is_available():
                    self.model = self.model.to(self.device)
                
                # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
                self.model.eval()
                
                print("âœ… æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ")
                print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
                print(f"   ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {sum(p.numel() for p in self.model.parameters())/1e6:.1f}M parameters")
                
            else:
                # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«
                if not TORCH_AVAILABLE:
                    print("âŒ PyTorchãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼AIã‚’ä½¿ç”¨ã—ã¾ã™")
                else:
                    print("ğŸ”§ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ãƒ€ãƒŸãƒ¼AIã‚’ä½¿ç”¨ã—ã¾ã™")
                self.tokenizer = None
                self.model = None
                
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
            self.use_real_model = False
            self.tokenizer = None
            self.model = None
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
        self.training_data_path = "data/conversations/training_data.json"
        
    async def generate_response(self, message: str, context: list) -> str:
        # éåŒæœŸã§CPUé›†ç´„çš„ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        return await asyncio.to_thread(self._generate_sync, message, context)
        
    def _generate_sync(self, message: str, context: list) -> str:
        if not self.use_real_model:
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã®ãƒ€ãƒŸãƒ¼å¿œç­”
            dummy_responses = [
                f"ã“ã‚“ã«ã¡ã¯ï¼ã€Œ{message}ã€ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã™ã­ã€‚",
                f"ãªã‚‹ã»ã©ã€ã€Œ{message}ã€ã§ã™ã­ã€‚èˆˆå‘³æ·±ã„ãŠè©±ã§ã™ã€‚",
                f"ã€Œ{message}ã€ã«ã¤ã„ã¦ã€ç§ãªã‚Šã«å›ç­”ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
                f"ã”è³ªå•ã®ã€Œ{message}ã€ã«é–¢ã—ã¦ã€ãŠç­”ãˆã—ã¾ã™ã€‚",
                f"ã€Œ{message}ã€ã«ã¤ã„ã¦ã€ä¸€ç·’ã«è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
            ]
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸã‚ˆã‚Šè‡ªç„¶ãªå¿œç­”
            if context:
                last_conv = context[-1] if context else None
                if last_conv:
                    dummy_responses.append(f"å…ˆã»ã©ã®ã€Œ{last_conv.get('user', '')}ã€ã®ä»¶ã‚‚å«ã‚ã¦ã€ã€Œ{message}ã€ã«ã¤ã„ã¦å›ç­”ã—ã¾ã™ã­ã€‚")
            
            return random.choice(dummy_responses)
        
        # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
        if not TORCH_AVAILABLE:
            return f"PyTorchãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã€Œ{message}ã€ã«ãƒ€ãƒŸãƒ¼å¿œç­”ã§ãŠç­”ãˆã—ã¾ã™ã€‚"
            
        try:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
            prompt = self._build_prompt(message, context)
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆæœ€å¤§é•·åˆ¶é™ï¼‰
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                max_length=512, 
                truncation=True
            ).to(self.device)
            
            # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ—¥æœ¬èªä¼šè©±æœ€é©åŒ–ï¼‰
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=100,  # ã‚ˆã‚ŠçŸ­ãã€å¿œç­”æ€§ã‚’å‘ä¸Š
                    min_new_tokens=10,   # æœ€ä½é™ã®é•·ã•ã‚’ä¿è¨¼
                    temperature=0.8,     # é©åº¦ãªå‰µé€ æ€§
                    do_sample=True,
                    top_p=0.85,          # è‡ªç„¶ãªå¿œç­”ã®ãƒãƒ©ãƒ³ã‚¹
                    top_k=40,            # èªå½™ã®å¤šæ§˜æ€§åˆ¶å¾¡
                    repetition_penalty=1.15,  # ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶å¼·åŒ–
                    no_repeat_ngram_size=2,   # n-gramç¹°ã‚Šè¿”ã—é˜²æ­¢
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰ã¨å¾Œå‡¦ç†
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.replace(prompt, "").strip()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            response = self._clean_response(response)
            
            return response if response else "ã™ã¿ã¾ã›ã‚“ã€ã†ã¾ãå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        except Exception as e:
            print(f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã‚ˆã‚Šè‡ªç„¶ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_responses = [
                f"ã€Œ{message}ã€ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã™ãŒã€ä»Šã¯å°‘ã—èª¿å­ãŒæ‚ªã„ã‚ˆã†ã§ã™ã€‚",
                f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œ{message}ã€ã«é–¢ã—ã¦ã€ä»Šã™ããŠç­”ãˆã§ãã¾ã›ã‚“ã€‚",
                f"ã€Œ{message}ã€ã«ã¤ã„ã¦ãŠèãã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚å°‘ã—æ™‚é–“ã‚’ãã ã•ã„ã€‚"
            ]
            return random.choice(fallback_responses)
        
    def _build_prompt(self, message: str, context: list) -> str:
        # æ—¥æœ¬èªä¼šè©±ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªæ—¥æœ¬èªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„ä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚\n\n"
        
        # æœ€æ–°ã®3ã¤ã®ä¼šè©±ã‚’å«ã‚ã‚‹ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        recent_context = context[-3:] if context else []
        for conv in recent_context:
            prompt += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {conv['user']}\n"
            prompt += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {conv['assistant']}\n"
            
        prompt += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "
        
        return prompt
    
    def _clean_response(self, response: str) -> str:
        """å¿œç­”ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¾Œå‡¦ç†"""
        if not response:
            return ""
        
        # ä¸è¦ãªæ”¹è¡Œã‚„ç©ºç™½ã‚’é™¤å»
        response = response.strip()
        
        # æ–‡ã®é€”ä¸­ã§åˆ‡ã‚Œã¦ã„ã‚‹å ´åˆã®å‡¦ç†
        sentences = response.split('ã€‚')
        if len(sentences) > 1 and sentences[-1].strip() and not sentences[-1].endswith(('ï¼', 'ï¼Ÿ', 'â™ª')):
            # æœ€å¾Œã®ä¸å®Œå…¨ãªæ–‡ã‚’é™¤å»
            response = 'ã€‚'.join(sentences[:-1]) + 'ã€‚'
        
        # çŸ­ã™ãã‚‹å¿œç­”ã®é™¤å¤–
        if len(response.replace(' ', '').replace('\n', '')) < 5:
            return ""
        
        # é‡è¤‡ãƒ•ãƒ¬ãƒ¼ã‚ºã®é™¤å»
        lines = response.split('\n')
        unique_lines = []
        for line in lines:
            if line.strip() and line.strip() not in unique_lines:
                unique_lines.append(line.strip())
        
        return '\n'.join(unique_lines) if unique_lines else response
        
    async def update_learning_data(self, user_id: str, message: str, response: str):
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        data = {
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "message": message,
            "response": response
        }
        
        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        if os.path.exists(self.training_data_path):
            with open(self.training_data_path, 'r', encoding='utf-8') as f:
                training_data = json.load(f)
        else:
            training_data = []
            
        training_data.append(data)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        os.makedirs(os.path.dirname(self.training_data_path), exist_ok=True)
        with open(self.training_data_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
            
        # ä¸€å®šé‡ã®ãƒ‡ãƒ¼ã‚¿ãŒæºœã¾ã£ãŸã‚‰è‡ªå‹•çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
        if len(training_data) % 100 == 0:
            asyncio.create_task(self._fine_tune_model())
            
    async def _fine_tune_model(self):
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        print("ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€LoRAã‚„QLoRAãªã©ã®åŠ¹ç‡çš„ãªæ‰‹æ³•ã‚’ä½¿ç”¨
        pass
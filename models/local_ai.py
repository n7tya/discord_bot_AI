"""
ãƒ­ãƒ¼ã‚«ãƒ«AIãƒ¢ãƒ‡ãƒ« - æ—¥æœ¬èªä¼šè©±AI

å®‰å…¨ã§è»½é‡ãªæ—¥æœ¬èªä¼šè©±AIã®å®Ÿè£…
"""

import asyncio
import json
import os
import random
from datetime import datetime
from typing import List, Dict, Optional

# PyTorchã¨Transformersã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®‰å…¨ã«è¡Œã†
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


class LocalAI:
    """ãƒ­ãƒ¼ã‚«ãƒ«æ—¥æœ¬èªAIãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self):
        """AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        from config import Config
        self.model_name = Config.AI_MODEL_NAME
        self.device = None
        self.tokenizer = None
        self.model = None
        self.use_real_model = False
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
        self.training_data_path = "data/conversations/training_data.json"
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã‚’è©¦è¡Œ
        self._initialize_model()

    def _initialize_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å‡¦ç†"""
        if not TORCH_AVAILABLE:
            print("âŒ PyTorchãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼AIã‚’ä½¿ç”¨ã—ã¾ã™")
            return

        try:
            # Mac Apple Silicon (M1/M2)ã®å ´åˆã¯MPSã€ãã‚Œä»¥å¤–ã¯CPU
            from config import Config
            if torch.backends.mps.is_available() and Config.AI_USE_MPS:
                self.device = torch.device("mps")
                print("ğŸ Apple Silicon (MPS) ã‚’ä½¿ç”¨ã—ã¾ã™")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                print("ğŸš€ CUDA GPU ã‚’ä½¿ç”¨ã—ã¾ã™")
            else:
                self.device = torch.device("cpu")
                print("ğŸ’» CPU ã‚’ä½¿ç”¨ã—ã¾ã™")
                
            print(f"æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« {self.model_name} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                use_fast=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆApple Siliconæœ€é©åŒ–ï¼‰
            model_kwargs = {
                "low_cpu_mem_usage": True,
                "use_cache": True
            }
            
            # Apple Silicon (MPS)ã®å ´åˆã®æœ€é©åŒ–
            if self.device.type == "mps":
                model_kwargs["torch_dtype"] = torch.float32  # MPSã¯float16ã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚
            elif self.device.type == "cuda":
                model_kwargs["torch_dtype"] = torch.float16
            else:
                model_kwargs["torch_dtype"] = torch.float32
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            self.model = self.model.to(self.device)
            self.model.eval()
            self.use_real_model = True
            
            print("âœ… æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ")
            print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
            print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**2:.1f}MB" if self.device.type == "cuda" else "")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
            self._reset_to_dummy_mode()
    def _reset_to_dummy_mode(self):
        """ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã«ãƒªã‚»ãƒƒãƒˆ"""
        self.use_real_model = False
        self.tokenizer = None
        self.model = None

    async def generate_response(self, message: str, context: List[Dict]) -> str:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ"""
        return await asyncio.to_thread(self._generate_sync, message, context)
        
    def _generate_sync(self, message: str, context: List[Dict]) -> str:
        """åŒæœŸçš„ãªå¿œç­”ç”Ÿæˆ"""
        # å…¥åŠ›ã®æ¤œè¨¼
        if not message or len(message.strip()) == 0:
            return "ä½•ã‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚"
        
        # é•·ã™ãã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆ¶é™
        if len(message) > 500:
            return "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã¾ã™ã€‚ã‚‚ã†å°‘ã—çŸ­ãã—ã¦ãã ã•ã„ã€‚"
        
        # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
        if not self.use_real_model:
            return self._generate_dummy_response(message, context)
        
        # å®Ÿéš›ã®AIãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆ
        try:
            return self._generate_real_response(message, context)
        except Exception as e:
            print(f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_dummy_response(message, context)

    def _generate_dummy_response(self, message: str, context: List[Dict]) -> str:
        """ãƒ€ãƒŸãƒ¼å¿œç­”ã®ç”Ÿæˆ"""
        responses = [
            f"ã€Œ{message}ã€ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã™ã­ã€‚",
            f"ãªã‚‹ã»ã©ã€ã€Œ{message}ã€ã§ã™ã­ã€‚",
            f"ã€Œ{message}ã€ã«é–¢ã—ã¦ãŠç­”ãˆã—ã¾ã™ã€‚",
            f"ã€Œ{message}ã€ã«ã¤ã„ã¦ä¸€ç·’ã«è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
        ]
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®
        if context and len(context) > 0:
            last_conv = context[-1]
            if 'user' in last_conv:
                responses.append(f"å…ˆã»ã©ã®è©±ã‚‚è¸ã¾ãˆã¦ã€ã€Œ{message}ã€ã«ã¤ã„ã¦ãŠç­”ãˆã—ã¾ã™ã­ã€‚")
        
        return random.choice(responses)

    def _generate_real_response(self, message: str, context: List[Dict]) -> str:
        """å®Ÿéš›ã®AIãƒ¢ãƒ‡ãƒ«ã§ã®å¿œç­”ç”Ÿæˆ"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompt = self._build_prompt(message, context)
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            max_length=400,  # ã‚ˆã‚ŠçŸ­ã„åˆ¶é™ã§å®‰å…¨æ€§å‘ä¸Š
            truncation=True
        ).to(self.device)
        
        # ã‚ˆã‚Šå®‰å…¨ã§åˆ¶å¾¡ã•ã‚ŒãŸç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        from config import Config
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                max_new_tokens=Config.AI_MAX_TOKENS,     # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                min_new_tokens=5,                        # æœ€ä½é™ã®é•·ã•
                temperature=Config.AI_TEMPERATURE,       # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                do_sample=True,
                top_p=0.8,               # ã‚ˆã‚Šåˆ¶é™çš„
                top_k=20,                # èªå½™ã‚’åˆ¶é™
                repetition_penalty=1.2,   # ç¹°ã‚Šè¿”ã—å¼·ãæŠ‘åˆ¶
                no_repeat_ngram_size=3,   # ã‚ˆã‚Šé•·ã„n-gramã®ç¹°ã‚Šè¿”ã—é˜²æ­¢
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                bad_words_ids=[[self.tokenizer.unk_token_id]] if hasattr(self.tokenizer, 'unk_token_id') else None
            )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰ã¨å¾Œå‡¦ç†
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        # å¿œç­”ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        response = self._clean_response(response)
        
        return response if response else "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã†ã¾ãå¿œç­”ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    def _build_prompt(self, message: str, context: List[Dict]) -> str:
        """å¯¾è©±ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        # ã‚ˆã‚Šåˆ¶å¾¡ã—ã‚„ã™ã„ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = ""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯æœ€æ–°ã®1ã¤ã®ã¿ï¼ˆæ··ä¹±ã‚’é¿ã‘ã‚‹ï¼‰
        if context and len(context) > 0:
            last_conv = context[-1]
            if 'user' in last_conv and 'assistant' in last_conv:
                prompt += f"å‰å›: {last_conv['user']} â†’ {last_conv['assistant']}\n"
        
        prompt += f"è³ªå•: {message}\nå›ç­”:"
        return prompt
    
    def _clean_response(self, response: str) -> str:
        """å¿œç­”ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        if not response:
            return ""
        
        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        response = response.strip()
        
        # æ”¹è¡Œã‚’é™¤å»
        response = response.replace('\n', ' ')
        
        # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã®ç©ºç™½ã«
        import re
        response = re.sub(r'\s+', ' ', response)
        
        # çŸ­ã™ãã‚‹å ´åˆã¯é™¤å¤–
        if len(response) < 3:
            return ""
        
        # é•·ã™ãã‚‹å ´åˆã¯æœ€åˆã®æ–‡ã®ã¿
        sentences = response.split('ã€‚')
        if len(sentences) > 0 and sentences[0].strip():
            response = sentences[0].strip()
            # æ–‡ã®çµ‚ã‚ã‚Šã«å¥ç‚¹ã‚’è¿½åŠ ï¼ˆãªã„å ´åˆï¼‰
            if not response.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                response += 'ã€‚'
        
        # ç•°å¸¸ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦é™¤å¤–
        if any(pattern in response.lower() for pattern in [
            'http', 'www.', 'chatroom', 'website', 'translation', 
            'contact me', 'support', 'blog', 'english'
        ]):
            return ""
        
        # 100æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(response) > 100:
            response = response[:100] + "..."
        
        return response
        
    async def update_learning_data(self, user_id: str, message: str, response: str):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
            data = {
                "user_id": user_id,
                "timestamp": datetime.now().isoformat(),
                "message": message[:100],  # é•·ã•åˆ¶é™
                "response": response[:200]  # é•·ã•åˆ¶é™
            }
            
            os.makedirs(os.path.dirname(self.training_data_path), exist_ok=True)
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            training_data = []
            if os.path.exists(self.training_data_path):
                with open(self.training_data_path, 'r', encoding='utf-8') as f:
                    training_data = json.load(f)
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            training_data.append(data)
            
            # æœ€æ–°ã®1000ä»¶ã®ã¿ä¿æŒ
            if len(training_data) > 1000:
                training_data = training_data[-1000:]
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            with open(self.training_data_path, 'w', encoding='utf-8') as f:
                json.dump(training_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")